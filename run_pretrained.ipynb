{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import json\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "TensorFlow version: 2.15.0\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "# MuseGAN Music Generator - Jupyter Notebook\n",
    "# Run each cell sequentially to generate music using pretrained MuseGAN models\n",
    "\n",
    "# Cell 1: Install Required Packages\n",
    "# Run this cell first to install all necessary dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "# Uncomment the line below if you need to install packages\n",
    "# install_packages()\n",
    "\n",
    "# Cell 2: Import Libraries and Set Up Environment\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable TensorFlow warnings for cleaner output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Set up matplotlib for better plotting in notebooks\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Cell 3: MuseGAN Generator Class Definition\n",
    "class MuseGANGenerator:\n",
    "    \"\"\"\n",
    "    Standalone MuseGAN music generator optimized for Jupyter notebooks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the MuseGAN generator.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the pretrained model directory or tar file\n",
    "            verbose: Whether to print detailed information\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.config = None\n",
    "        self.sess = None\n",
    "        self.graph = None\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Track names for visualization\n",
    "        self.track_names = ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings']\n",
    "        self.track_colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        \n",
    "        # Music generation parameters\n",
    "        self.n_tracks = 5\n",
    "        self.n_bars = 4\n",
    "        self.n_steps_per_bar = 24\n",
    "        self.n_pitches = 128\n",
    "        self.lowest_pitch = 24\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"🎵 MuseGAN Generator initialized!\")\n",
    "    \n",
    "    def extract_model(self, tar_path: str) -> str:\n",
    "        \"\"\"Extract the tar file containing the pretrained model.\"\"\"\n",
    "        extract_dir = tar_path.replace('.tar', '_extracted')\n",
    "        \n",
    "        if not os.path.exists(extract_dir):\n",
    "            if self.verbose:\n",
    "                print(f\"📦 Extracting {tar_path} to {extract_dir}...\")\n",
    "            with tarfile.open(tar_path, 'r') as tar:\n",
    "                tar.extractall(extract_dir)\n",
    "            if self.verbose:\n",
    "                print(\"✅ Extraction complete!\")\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(f\"📁 Using existing extracted directory: {extract_dir}\")\n",
    "        \n",
    "        return extract_dir\n",
    "    \n",
    "    def load_model(self) -> bool:\n",
    "        \"\"\"Load the pretrained MuseGAN model.\"\"\"\n",
    "        model_dir = self.model_path\n",
    "        \n",
    "        # If it's a tar file, extract it first\n",
    "        if self.model_path.endswith('.tar'):\n",
    "            model_dir = self.extract_model(self.model_path)\n",
    "        \n",
    "        # Look for model files\n",
    "        ckpt_files = []\n",
    "        config_file = None\n",
    "        \n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.meta'):\n",
    "                    ckpt_files.append(os.path.join(root, file.replace('.meta', '')))\n",
    "                elif file == 'config.json':\n",
    "                    config_file = os.path.join(root, file)\n",
    "        \n",
    "        if not ckpt_files:\n",
    "            if self.verbose:\n",
    "                print(\"❌ No checkpoint files found. Will create fallback generator.\")\n",
    "            return False\n",
    "        \n",
    "        # Load configuration if available\n",
    "        if config_file and os.path.exists(config_file):\n",
    "            with open(config_file, 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "                if self.verbose:\n",
    "                    print(f\"📄 Loaded configuration: {config_file}\")\n",
    "        \n",
    "        # Use the most recent checkpoint\n",
    "        ckpt_path = ckpt_files[0]\n",
    "        if self.verbose:\n",
    "            print(f\"🔄 Loading model from: {ckpt_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Create TensorFlow session and load model\n",
    "            tf.compat.v1.disable_eager_execution()\n",
    "            self.sess = tf.compat.v1.Session()\n",
    "            \n",
    "            # Import the meta graph\n",
    "            saver = tf.compat.v1.train.import_meta_graph(ckpt_path + '.meta')\n",
    "            saver.restore(self.sess, ckpt_path)\n",
    "            \n",
    "            # Get the default graph\n",
    "            self.graph = tf.compat.v1.get_default_graph()\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(\"✅ Model loaded successfully!\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"❌ Error loading model: {e}\")\n",
    "                print(\"🔄 Will create fallback generator.\")\n",
    "            return False\n",
    "    \n",
    "    def create_generator_network(self, z_dim: int = 32):\n",
    "        \"\"\"Create a simple generator network for music generation.\"\"\"\n",
    "        # Input noise vector\n",
    "        z = tf.compat.v1.placeholder(tf.float32, [None, z_dim], name='z')\n",
    "        \n",
    "        # Generator architecture\n",
    "        with tf.compat.v1.variable_scope('generator'):\n",
    "            # Dense layers\n",
    "            h1 = tf.compat.v1.layers.dense(z, 1024, activation=tf.nn.relu)\n",
    "            h2 = tf.compat.v1.layers.dense(h1, 2048, activation=tf.nn.relu)\n",
    "            \n",
    "            # Reshape for convolutional layers\n",
    "            h2_reshaped = tf.reshape(h2, [-1, 1, 1, 2048])\n",
    "            \n",
    "            # Transpose convolutions to generate music\n",
    "            conv1 = tf.compat.v1.layers.conv2d_transpose(\n",
    "                h2_reshaped, 512, (1, 4), strides=(1, 2), padding='same', activation=tf.nn.relu\n",
    "            )\n",
    "            conv2 = tf.compat.v1.layers.conv2d_transpose(\n",
    "                conv1, 256, (1, 4), strides=(1, 2), padding='same', activation=tf.nn.relu\n",
    "            )\n",
    "            conv3 = tf.compat.v1.layers.conv2d_transpose(\n",
    "                conv2, 128, (1, 4), strides=(1, 3), padding='same', activation=tf.nn.relu\n",
    "            )\n",
    "            \n",
    "            # Final layer to generate music\n",
    "            output = tf.compat.v1.layers.conv2d_transpose(\n",
    "                conv3, self.n_tracks * self.n_pitches, (4, 4), \n",
    "                strides=(1, 1), padding='same', activation=tf.nn.sigmoid\n",
    "            )\n",
    "            \n",
    "            # Reshape to music tensor format\n",
    "            music_output = tf.reshape(output, \n",
    "                [-1, self.n_bars, self.n_steps_per_bar, self.n_tracks, self.n_pitches])\n",
    "        \n",
    "        return z, music_output\n",
    "    \n",
    "    def generate_music(self, n_samples: int = 4, temperature: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"Generate music samples with progress indication.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"🎼 Generating {n_samples} music samples...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to find generator output in the loaded graph\n",
    "            try:\n",
    "                # Look for common tensor names\n",
    "                possible_inputs = ['z:0', 'noise:0', 'input:0', 'generator/input:0']\n",
    "                possible_outputs = ['generator/output:0', 'generated_music:0', 'fake_data:0']\n",
    "                \n",
    "                input_tensor = None\n",
    "                output_tensor = None\n",
    "                \n",
    "                for name in possible_inputs:\n",
    "                    try:\n",
    "                        input_tensor = self.graph.get_tensor_by_name(name)\n",
    "                        break\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                \n",
    "                for name in possible_outputs:\n",
    "                    try:\n",
    "                        output_tensor = self.graph.get_tensor_by_name(name)\n",
    "                        break\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                \n",
    "                if input_tensor is None or output_tensor is None:\n",
    "                    raise KeyError(\"Could not find input/output tensors\")\n",
    "                \n",
    "                # Generate random noise\n",
    "                z_dim = input_tensor.shape[-1]\n",
    "                noise = np.random.normal(0, temperature, (n_samples, z_dim))\n",
    "                \n",
    "                # Generate music\n",
    "                generated_music = self.sess.run(output_tensor, feed_dict={input_tensor: noise})\n",
    "                \n",
    "            except KeyError:\n",
    "                if self.verbose:\n",
    "                    print(\"🔄 Creating fallback generator...\")\n",
    "                # Fallback: create our own generator\n",
    "                z_input, music_output = self.create_generator_network()\n",
    "                \n",
    "                # Initialize variables\n",
    "                init = tf.compat.v1.global_variables_initializer()\n",
    "                self.sess.run(init)\n",
    "                \n",
    "                # Generate random noise\n",
    "                noise = np.random.normal(0, temperature, (n_samples, 32))\n",
    "                \n",
    "                # Generate music\n",
    "                generated_music = self.sess.run(music_output, feed_dict={z_input: noise})\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"✅ Generated music shape: {generated_music.shape}\")\n",
    "            return generated_music\n",
    "            \n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                print(f\"❌ Error generating music: {e}\")\n",
    "                print(\"🎲 Using random music as fallback...\")\n",
    "            return self.generate_random_music(n_samples)\n",
    "    \n",
    "    def generate_random_music(self, n_samples: int) -> np.ndarray:\n",
    "        \"\"\"Generate random music as a fallback.\"\"\"\n",
    "        return np.random.random((n_samples, self.n_bars, self.n_steps_per_bar, \n",
    "                               self.n_tracks, self.n_pitches))\n",
    "    \n",
    "    def postprocess_music(self, music: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"Postprocess generated music.\"\"\"\n",
    "        # Apply threshold to create binary piano roll\n",
    "        binary_music = (music > threshold).astype(np.float32)\n",
    "        \n",
    "        # Remove notes below the lowest pitch\n",
    "        binary_music[:, :, :, :, :self.lowest_pitch] = 0\n",
    "        \n",
    "        return binary_music\n",
    "    \n",
    "    def music_to_pianoroll(self, music: np.ndarray, track_idx: int = 1) -> np.ndarray:\n",
    "        \"\"\"Convert music tensor to piano roll for visualization.\"\"\"\n",
    "        n_bars, n_steps_per_bar, n_tracks, n_pitches = music.shape\n",
    "        total_steps = n_bars * n_steps_per_bar\n",
    "        \n",
    "        # Extract the specific track\n",
    "        track_data = music[:, :, track_idx, :]\n",
    "        \n",
    "        # Reshape to [total_steps, n_pitches]\n",
    "        pianoroll = track_data.reshape(total_steps, n_pitches)\n",
    "        \n",
    "        return pianoroll\n",
    "    \n",
    "    def plot_pianoroll(self, music: np.ndarray, sample_idx: int = 0, \n",
    "                      track_idx: int = 1, figsize: tuple = (15, 8)):\n",
    "        \"\"\"Plot piano roll visualization.\"\"\"\n",
    "        pianoroll = self.music_to_pianoroll(music[sample_idx], track_idx)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Create the plot\n",
    "        im = ax.imshow(pianoroll.T, aspect='auto', origin='lower', \n",
    "                      cmap='Blues', interpolation='nearest')\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax.set_title(f'{self.track_names[track_idx]} - Sample {sample_idx}', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Time Steps', fontsize=12)\n",
    "        ax.set_ylabel('MIDI Pitch', fontsize=12)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax, label='Note Velocity')\n",
    "        \n",
    "        # Add grid for bars\n",
    "        for bar in range(1, self.n_bars):\n",
    "            ax.axvline(x=bar * self.n_steps_per_bar, color='red', \n",
    "                      linestyle='--', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_track_comparison(self, music: np.ndarray, sample_idx: int = 0, \n",
    "                            figsize: tuple = (20, 12)):\n",
    "        \"\"\"Plot comparison of all tracks.\"\"\"\n",
    "        fig, axes = plt.subplots(self.n_tracks, 1, figsize=figsize, sharex=True)\n",
    "        \n",
    "        for track_idx in range(self.n_tracks):\n",
    "            pianoroll = self.music_to_pianoroll(music[sample_idx], track_idx)\n",
    "            \n",
    "            im = axes[track_idx].imshow(pianoroll.T, aspect='auto', origin='lower', \n",
    "                                      cmap='Blues', interpolation='nearest')\n",
    "            \n",
    "            axes[track_idx].set_title(f'{self.track_names[track_idx]}', \n",
    "                                    fontsize=14, fontweight='bold')\n",
    "            axes[track_idx].set_ylabel('MIDI Pitch', fontsize=10)\n",
    "            \n",
    "            # Add bar lines\n",
    "            for bar in range(1, self.n_bars):\n",
    "                axes[track_idx].axvline(x=bar * self.n_steps_per_bar, \n",
    "                                       color='red', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        axes[-1].set_xlabel('Time Steps', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_music_statistics(self, music: np.ndarray) -> dict:\n",
    "        \"\"\"Analyze and return statistics about the generated music.\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for i, track_name in enumerate(self.track_names[:music.shape[3]]):\n",
    "            track_data = music[0, :, :, i, :]\n",
    "            active_notes = np.sum(track_data > 0.5)\n",
    "            total_notes = track_data.size\n",
    "            density = active_notes / total_notes * 100\n",
    "            \n",
    "            # Pitch range analysis\n",
    "            active_pitches = np.where(np.sum(track_data, axis=(0, 1)) > 0)[0]\n",
    "            pitch_range = (active_pitches.min(), active_pitches.max()) if len(active_pitches) > 0 else (0, 0)\n",
    "            \n",
    "            stats[track_name] = {\n",
    "                'active_notes': active_notes,\n",
    "                'total_notes': total_notes,\n",
    "                'density': density,\n",
    "                'pitch_range': pitch_range,\n",
    "                'unique_pitches': len(active_pitches)\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def save_as_midi(self, music: np.ndarray, filename: str, tempo: int = 120):\n",
    "        \"\"\"Save generated music as MIDI file.\"\"\"\n",
    "        try:\n",
    "            import pretty_midi\n",
    "            \n",
    "            # Create a MIDI object\n",
    "            midi = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "            \n",
    "            # Process each track\n",
    "            for track_idx in range(min(self.n_tracks, music.shape[3])):\n",
    "                # Create instrument\n",
    "                instrument = pretty_midi.Instrument(\n",
    "                    program=0 if track_idx == 0 else track_idx * 8,\n",
    "                    is_drum=(track_idx == 0),\n",
    "                    name=self.track_names[track_idx] if track_idx < len(self.track_names) else f'Track_{track_idx}'\n",
    "                )\n",
    "                \n",
    "                # Convert to piano roll\n",
    "                pianoroll = self.music_to_pianoroll(music[0], track_idx)\n",
    "                \n",
    "                # Convert piano roll to MIDI notes\n",
    "                for step, pitches in enumerate(pianoroll):\n",
    "                    for pitch, velocity in enumerate(pitches):\n",
    "                        if velocity > 0:\n",
    "                            # Calculate timing\n",
    "                            start_time = step * (60.0 / tempo / 6)\n",
    "                            end_time = start_time + (60.0 / tempo / 6)\n",
    "                            \n",
    "                            # Create note\n",
    "                            note = pretty_midi.Note(\n",
    "                                velocity=int(velocity * 127),\n",
    "                                pitch=pitch,\n",
    "                                start=start_time,\n",
    "                                end=end_time\n",
    "                            )\n",
    "                            instrument.notes.append(note)\n",
    "                \n",
    "                midi.instruments.append(instrument)\n",
    "            \n",
    "            # Save MIDI file\n",
    "            midi.write(filename)\n",
    "            if self.verbose:\n",
    "                print(f\"🎵 MIDI file saved as: {filename}\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"❌ pretty_midi not installed. Cannot save as MIDI.\")\n",
    "            print(\"Install with: !pip install pretty_midi\")\n",
    "    \n",
    "    def save_as_numpy(self, music: np.ndarray, filename: str):\n",
    "        \"\"\"Save generated music as numpy array.\"\"\"\n",
    "        np.save(filename, music)\n",
    "        if self.verbose:\n",
    "            print(f\"💾 Music saved as numpy array: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing MuseGAN generator...\n",
      "🎵 MuseGAN Generator initialized!\n"
     ]
    }
   ],
   "source": [
    "# Update this path to point to your pretrained model\n",
    "MODEL_PATH = \"DL_music_MouseGAN/pretrained_models.tar\"  # Change this to your model path\n",
    "\n",
    "# Generation parameters\n",
    "N_SAMPLES = 4\n",
    "TEMPERATURE = 1.0\n",
    "THRESHOLD = 0.5\n",
    "OUTPUT_DIR = \"./generated_music\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize generator\n",
    "print(\"🚀 Initializing MuseGAN generator...\")\n",
    "generator = MuseGANGenerator(MODEL_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading pretrained model...\n",
      "📦 Extracting DL_music_MouseGAN/pretrained_models.tar to DL_music_MouseGAN/pretrained_models_extracted...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DL_music_MouseGAN/pretrained_models.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 Loading pretrained model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_loaded:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 96\u001b[0m, in \u001b[0;36mMuseGANGenerator.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# If it's a tar file, extract it first\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m     model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Look for model files\u001b[39;00m\n\u001b[0;32m     99\u001b[0m ckpt_files \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m, in \u001b[0;36mMuseGANGenerator.extract_model\u001b[1;34m(self, tar_path)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📦 Extracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtar_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m     81\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextractall(extract_dir)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tarfile.py:1632\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m     saved_pos \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ReadError, CompressionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1634\u001b[0m     error_msgs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- method \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomptype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tarfile.py:1698\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip module is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1698\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DL_music_MouseGAN/pretrained_models.tar'"
     ]
    }
   ],
   "source": [
    "print(\"🔄 Loading pretrained model...\")\n",
    "model_loaded = generator.load_model()\n",
    "\n",
    "if model_loaded:\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "else:\n",
    "    print(\"⚠️  Using fallback generator (model not found or failed to load)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🎼 Generating {N_SAMPLES} music samples...\")\n",
    "print(f\"Parameters: Temperature={TEMPERATURE}, Threshold={THRESHOLD}\")\n",
    "\n",
    "# Generate music\n",
    "generated_music = generator.generate_music(\n",
    "    n_samples=N_SAMPLES,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "# Postprocess music\n",
    "processed_music = generator.postprocess_music(generated_music, THRESHOLD)\n",
    "\n",
    "print(\"✅ Music generation complete!\")\n",
    "print(f\"Generated music shape: {processed_music.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Analyzing generated music...\")\n",
    "\n",
    "# Get statistics\n",
    "stats = generator.analyze_music_statistics(processed_music)\n",
    "\n",
    "# Display statistics in a nice format\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎵 GENERATED MUSIC STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for track_name, track_stats in stats.items():\n",
    "    print(f\"\\n🎹 {track_name.upper()}:\")\n",
    "    print(f\"   Active Notes: {track_stats['active_notes']:,}\")\n",
    "    print(f\"   Density: {track_stats['density']:.1f}%\")\n",
    "    print(f\"   Pitch Range: {track_stats['pitch_range'][0]} - {track_stats['pitch_range'][1]}\")\n",
    "    print(f\"   Unique Pitches: {track_stats['unique_pitches']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎨 Creating visualizations...\")\n",
    "\n",
    "# Plot individual track (Piano by default)\n",
    "print(\"\\n📊 Piano Roll Visualization (Piano Track):\")\n",
    "generator.plot_pianoroll(processed_music, sample_idx=0, track_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 All Tracks Comparison:\")\n",
    "generator.plot_track_comparison(processed_music, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Saving generated music...\")\n",
    "\n",
    "# Save each sample\n",
    "for i in range(N_SAMPLES):\n",
    "    sample_music = processed_music[i:i+1]\n",
    "    \n",
    "    # Save as numpy\n",
    "    numpy_path = os.path.join(OUTPUT_DIR, f'generated_music_sample_{i}.npy')\n",
    "    generator.save_as_numpy(sample_music, numpy_path)\n",
    "    \n",
    "    # Save as MIDI\n",
    "    midi_path = os.path.join(OUTPUT_DIR, f'generated_music_sample_{i}.mid')\n",
    "    generator.save_as_midi(sample_music, midi_path)\n",
    "\n",
    "print(f\"\\n✅ All files saved to: {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmusic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
